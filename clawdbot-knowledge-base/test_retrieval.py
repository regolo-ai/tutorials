#!/usr/bin/env python3
"""
Test script for RAG pipeline quality and performance
Run: python3 test_retrieval.py
"""

import time
from rag_pipeline import KnowledgeBaseRAG

def test_retrieval_quality():
    """Test retrieval accuracy with sample queries."""
    
    print("\nğŸ§ª Testing RAG Pipeline Quality & Performance\n")
    print("="*60)
    
    # Initialize RAG
    rag = KnowledgeBaseRAG()
    if not rag.load_index():
        print("âŒ No index found. Build one first:")
        print("   python3 rag_pipeline.py build")
        return
    
    # Sample test queries (customize for your knowledge base)
    test_queries = [
        "What is our GDPR data retention policy?",
        "How do we handle production incidents?",
        "What are the deployment procedures?",
        "Explain our security guidelines",
        "What is the process for code review?",
    ]
    
    results = []
    total_latency = 0
    total_cost = 0
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n{'â”€'*60}")
        print(f"ğŸ“‹ Test {i}/{len(test_queries)}: {query}")
        print(f"{'â”€'*60}")
        
        t0 = time.time()
        result = rag.query(query)
        query_time = (time.time() - t0) * 1000
        
        total_latency += query_time
        total_cost += result['cost_eur']
        
        print(f"\nğŸ“š Answer:\n{result['answer'][:200]}...")
        print(f"\nğŸ“ Top Sources:")
        for src in result['sources'][:3]:
            print(f"   â€¢ {src['file']} (relevance: {src['relevance']})")
        
        print(f"\nâ±ï¸  Latency: {query_time:.0f}ms")
        print(f"ğŸ’° Cost: â‚¬{result['cost_eur']:.4f}")
        print(f"ğŸ” Retrieved: {result['retrieval_candidates']} â†’ Reranked: {result['reranked_chunks']}")
        
        results.append({
            'query': query,
            'latency_ms': query_time,
            'cost_eur': result['cost_eur'],
            'sources': len(result['sources'])
        })
    
    # Summary statistics
    print(f"\n{'='*60}")
    print("ğŸ“Š SUMMARY STATISTICS")
    print(f"{'='*60}")
    
    avg_latency = total_latency / len(test_queries)
    avg_cost = total_cost / len(test_queries)
    
    latencies = [r['latency_ms'] for r in results]
    p95_latency = sorted(latencies)[int(len(latencies) * 0.95)]
    
    print(f"\nâ±ï¸  Latency:")
    print(f"   â”œâ”€ Average: {avg_latency:.0f}ms")
    print(f"   â”œâ”€ p95: {p95_latency:.0f}ms")
    print(f"   â”œâ”€ Min: {min(latencies):.0f}ms")
    print(f"   â””â”€ Max: {max(latencies):.0f}ms")
    
    print(f"\nğŸ’° Cost:")
    print(f"   â”œâ”€ Average per query: â‚¬{avg_cost:.4f}")
    print(f"   â”œâ”€ Total (5 queries): â‚¬{total_cost:.4f}")
    print(f"   â””â”€ Projected (1K queries): â‚¬{avg_cost * 1000:.2f}")
    
    print(f"\nğŸ“š Knowledge Base:")
    print(f"   â”œâ”€ Documents: {rag.index['num_docs']}")
    print(f"   â”œâ”€ Chunks: {rag.index['num_chunks']}")
    print(f"   â””â”€ Models: {rag.index['models']['embed']}, {rag.index['models']['rerank']}, {rag.index['models']['chat']}")
    
    print(f"\n{'='*60}")
    print("âœ… Testing complete!")
    print(f"{'='*60}\n")


if __name__ == "__main__":
    test_retrieval_quality()
